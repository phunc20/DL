{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "K = keras.backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crooked_sigmoid(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Our customized loss function\n",
    "    Example: (4-class classification)\n",
    "        y_pred = [ -500.0, -37.92, 99.99]\n",
    "        y_true = [0, 0, 0]  # meaning that the true class is the fourth one.\n",
    "    \"\"\"\n",
    "    # I guess y_pred.shape equals (m, 9) for Fashion MNIST\n",
    "    s = tf.math.reduce_sum(y_pred, axis=1)\n",
    "    #print(f\"s.shape = {s.shape}\")\n",
    "    last_proba = 1 - tf.math.sigmoid(s)\n",
    "    #print(f\"tf.math.sigmoid(s) = {tf.math.sigmoid(s)}\")\n",
    "    #print(f\"tf.math.sigmoid(s).shape = {tf.math.sigmoid(s).shape}\")\n",
    "    #print(f\"last_proba = 1 - tf.math.sigmoid(s) = {last_proba}\")\n",
    "    #print(f\"last_proba.shape = {last_proba.shape}\")\n",
    "    #first_probas = tf.math.sigmoid(s) * tf.nn.softmax(y_pred)\n",
    "    #print()\n",
    "    #first_probas = tf.stack([tf.math.sigmoid(s)]*tf.cast(tf.shape(y_pred)[1], tf.float32) , axis=1) * tf.nn.softmax(y_pred)\n",
    "    #sigmoid_s_good_shape = tf.stack([tf.math.sigmoid(s)]*tf.cast(tf.shape(y_pred)[1], tf.float32) , axis=1)\n",
    "    #print(f\"tf.shape(y_pred) = {tf.shape(y_pred)}\")\n",
    "    #print(f\"tf.shape(y_pred)[1] = {tf.shape(y_pred)[1]}\")\n",
    "    #print(f\"tf.math.sigmoid(s) = {tf.math.sigmoid(s)}\")\n",
    "    #print(f\"[tf.math.sigmoid(s)]*tf.shape(y_pred)[1] = {[tf.math.sigmoid(s)]*tf.shape(y_pred)[1]}\")\n",
    "    #print(f\"[tf.math.sigmoid(s)]*tf.shape(y_pred)[1].numpy() = {[tf.math.sigmoid(s)]*tf.shape(y_pred)[1].numpy()}\")\n",
    "    \n",
    "    #sigmoid_s_good_shape = tf.stack([tf.math.sigmoid(s)]*tf.shape(y_pred)[1].numpy(), axis=1)\n",
    "    #sigmoid_s_good_shape = tf.stack([tf.math.sigmoid(s)]*y_pred.numpy().shape[1], axis=1)\n",
    "    #sigmoid_s_good_shape = tf.stack([tf.math.sigmoid(s)]*K.eval(y_pred).shape[1], axis=1)\n",
    "    sigmoid_s_good_shape = tf.stack([tf.math.sigmoid(s)]*y_pred.shape[1], axis=1)\n",
    "    \n",
    "    #print(f\"sigmoid_s_good_shape = {sigmoid_s_good_shape}\")\n",
    "    #print(f\"sigmoid_s_good_shape.shape = {sigmoid_s_good_shape.shape}\")\n",
    "    #print(f\"tf.cast(tf.shape(y_pred)[1], tf.float32) = {tf.cast(tf.shape(y_pred)[1], tf.float32)}\")\n",
    "    first_probas = sigmoid_s_good_shape * tf.nn.softmax(y_pred)\n",
    "    #print(f\"y_pred[1] = {y_pred[1]}\")\n",
    "    #print(f\"tf.nn.softmax(y_pred)[1] = {tf.nn.softmax(y_pred)[1]}\")\n",
    "    #print(f\"first_probas = {first_probas}\")\n",
    "    #print(f\"first_probas[1] = {first_probas[1]}\")\n",
    "    #print(f\"first_probas.shape = {first_probas.shape}\")\n",
    "    #yy_pred = tf.concat([first_probas, [last_proba]], axis=0)\n",
    "    #print(f\"last_proba.shape = {last_proba.shape}\")\n",
    "    yy_pred = tf.concat([first_probas, last_proba[...,tf.newaxis]], axis=1)\n",
    "    #print(f\"yy_pred.shape = {yy_pred.shape}\")\n",
    "    print(f\"yy_pred =\\n{yy_pred}\")\n",
    "    #print(f\"yy_true =\\n{yy_true}\")\n",
    "    #print(f\"tf.math.reduce_sum((yy_pred[:3], axis=1) = {tf.math.reduce_sum(yy_pred[:3], axis=1)}\")\n",
    "    ## Wrong!\n",
    "    #yy_true = tf.cond(tf.math.reduce_sum(y_true, axis=1) < 1,\n",
    "    #                  true_fn=lambda: tf.concat([y_true, [1]], 0),\n",
    "    #                  false_fn=lambda: tf.concat([y_true, [0]], 0),\n",
    "    #)\n",
    "    yy_true = tf.concat([y_true, (1- tf.math.reduce_sum(y_true, axis=1))[...,tf.newaxis]], axis=1)\n",
    "    #print(f\"yy_true[:4] =\\n{yy_true[:4]}\")\n",
    "    #return keras.metrics.categorical_crossentropy(\n",
    "    #return keras.losses.categorical_crossentropy(\n",
    "    #    yy_true,\n",
    "    #    yy_pred,\n",
    "    #    from_logits=False,\n",
    "    #)\n",
    "    return tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=yy_true,\n",
    "        logits=yy_pred,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nn = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(9)\n",
    "])\n",
    "\n",
    "model_nn.compile(optimizer='adam',\n",
    "              loss=crooked_sigmoid,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "yy_pred =\n",
      "Tensor(\"crooked_sigmoid/concat:0\", shape=(32, 10), dtype=float32)\n",
      "yy_pred =\n",
      "Tensor(\"crooked_sigmoid/concat:0\", shape=(32, 10), dtype=float32)\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 1.7106 - accuracy: 0.6804\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 1.6245 - accuracy: 0.7549\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 1.6103 - accuracy: 0.7644\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 1.6030 - accuracy: 0.7711\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5978 - accuracy: 0.7752\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 1.5937 - accuracy: 0.7795\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.5899 - accuracy: 0.7821\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.5844 - accuracy: 0.7876\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.5827 - accuracy: 0.7883\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 1.5803 - accuracy: 0.7907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f417ba89f40>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nn.fit(train_images, tf.one_hot(train_labels, depth=10)[:,:-1], epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it because I implemented **wrongly** the loss function? Maybe the last return value should not be `tf.nn.softmax_cross_entropy_with_logits`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "help(keras.metrics.categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "help(keras.losses.categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "help(keras.losses.CategoricalCrossentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(0,10,size=(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arti_true = np.random.rand(6000,9)\n",
    "r = np.random.randint(0,9,size=(6,))\n",
    "arti_true[range(6), r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arti_true[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_instances = 6000\n",
    "n_classes = 9\n",
    "arti_true = np.zeros((n_instances, n_classes))\n",
    "arti_true[range(n_instances), np.random.randint(0, n_classes, size=(n_instances,))]\n",
    "arti_pred = np.random.rand(*(arti_true.shape))\n",
    "arti_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crooked_sigmoid(arti_true, arti_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crooked_sigmoid(arti_true, arti_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crooked_sigmoid(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Our customized loss function\n",
    "    Example: (4-class classification)\n",
    "        y_pred = [ -500.0, -37.92, 99.99]\n",
    "        y_true = [0, 0, 0]  # meaning that the true class is the fourth one.\n",
    "    \"\"\"\n",
    "    # I guess y_pred.shape equals (m, 9) for Fashion MNIST\n",
    "    s = tf.math.reduce_sum(y_pred, axis=1)\n",
    "    last_proba = 1 - tf.math.sigmoid(s)\n",
    "    sigmoid_s_good_shape = tf.stack([tf.math.sigmoid(s)]*y_pred.shape[1], axis=1)\n",
    "    first_probas = sigmoid_s_good_shape * tf.nn.softmax(y_pred)\n",
    "    yy_pred = tf.concat([first_probas, last_proba[...,tf.newaxis]], axis=1)\n",
    "    print(f\"yy_pred =\\n{yy_pred}\")\n",
    "    yy_true = tf.concat([y_true, (1- tf.math.reduce_sum(y_true, axis=1))[...,tf.newaxis]], axis=1)\n",
    "    print(f\"yy_true =\\n{yy_true}\")\n",
    "    #return tf.nn.softmax_cross_entropy_with_logits(\n",
    "    #    labels=yy_true,\n",
    "    #    logits=yy_pred,\n",
    "    #)\n",
    "    #return keras.metrics.categorical_crossentropy(\n",
    "    return keras.losses.categorical_crossentropy(\n",
    "        yy_true,\n",
    "        yy_pred,\n",
    "        from_logits=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crooked_sigmoid(arti_true, arti_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Still buggy?\n",
    "The loss in the cell above **shouldn't they be all zeros** (because we have already let `y_pred = y_true`)?\n",
    "\n",
    "**No, actually not. Cf. below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crooked_sigmoid(arti_true, -999*np.ones((n_instances, n_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's investigate the layers' activation functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "?keras.layers.Dense"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  activation: Activation function to use.\n",
    "    If you don't specify anything, no activation is applied\n",
    "    (ie. \"linear\" activation: `a(x) = x`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let's try remove the `relu` in the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    #keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(128),\n",
    "    ## Try adding another layer, making it deeper.\n",
    "    #keras.layers.Dense(128),\n",
    "    keras.layers.Dense(9)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=crooked_sigmoid,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_images, tf.one_hot(train_labels, depth=10)[:,:-1], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "8/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a customized loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
